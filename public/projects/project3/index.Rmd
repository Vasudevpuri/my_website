---
title: "Climate Change Analysis - Is it really a hoax?"
author: "Vasu Dev Puri"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---


```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     
  size="small")   
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, include=FALSE}
library(tidyverse)  
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(httr)
library(readxl)
library(vroom)
library(kableExtra)
library(lubridate)
library(infer)
```

# Climate change and temperature anomalies 

In the first section of this report we will be studying climate change by looking at temperature anomalies recorded since 1880, specifically the deviations from the expected temperature. The raw data is the *Combined Land-Surface Air and Sea-Surface Water Temperature Anomalies* in the Northern Hemisphere at [NASA's Goddard Institute for Space Studies](https://data.giss.nasa.gov/gistemp). The [tabular data of temperature anomalies can be found here.](https://data.giss.nasa.gov/gistemp/tabledata_v3/NH.Ts+dSST.txt)

```{r weather_data, cache=TRUE}

weather <- 
  read_csv("https://data.giss.nasa.gov/gistemp/tabledata_v3/NH.Ts+dSST.csv", 
           skip = 1, 
           na = "***")

```

```{r tidyweather}
#remove unnecessary columns from the original data frame
remove_cols <- weather %>%
  select(1:13)

tidyweather <- remove_cols %>%
  pivot_longer(cols = 2:13, names_to = "Month", values_to = "delta")

```

## Plotting Information

After tidying the data, we plot it in a time-series scatter plot below. 

```{r scatter_plot}

tidyweather <- tidyweather %>%
  mutate(date = ymd(paste(as.character(Year), Month, "1")),
         month = month(date, label=TRUE),
         year = year(date))

ggplot(tidyweather, aes(x=date, y = delta))+
  geom_point(size = 0.5)+
  geom_smooth(color="red") +
  labs (
    title = "The planet is getting warmer!", subtitle = "Weather anomalies since 1880", x = "Year", y = "Temperature Deviation", caption = "Source: NASA's Goddard Institute for Space Studies"
  ) + 
  theme_minimal()

```

By further analysing the data below we can observe that the effect of increasing temperature is more pronounced in some months. In fact, evident from both the follwing scatter plots and summary statistics, we see that October has the highest mean delta, indicating that it has a more pronounced overall increase in temperature compared to other months. Months at the start of the year (Jan, Feb, Mar) also have larger variation in delta compared to other months.


```{r facet_wrap}
#faceting data per month 
ggplot(tidyweather, aes(x=date, y = delta))+
  geom_point(size = 0.3)+
  geom_smooth(color = "red") +
  theme_bw() +
  facet_wrap(~month) +
  labs (title = "October has become the month with the most unexpected weather", subtitle = "Weather Anomalies since 1880 by month", x = "Year", y = "Temperature Deviation", caption = "Source: NASA's Goddard Institute for Space Studies") 

insights_avgdelta <- tidyweather %>%
  group_by(Month) %>%
  #summary statistics for our deltas 
  summarize(mean = mean(delta, 
  #ignore missing values 
  na.rm = TRUE)) 

#formatting table 
library(kableExtra)
insights_avgdelta %>%
  arrange(desc(mean)) %>%
  kbl(caption = "Mean Temperature Deviation per month") %>%
  kable_styling()

```
```{r stdev_deltas}

insights_sddelta <- tidyweather %>%
  group_by(Month) %>%
  #summary statistics for our deltas 
  summarize("sd" = sd(delta, na.rm = TRUE)) 

#formatting table
library(kableExtra)
insights_sddelta %>%
  arrange(desc(sd)) %>%
  kbl(caption = "Standard Deviation of Temperature Anomalies per month") %>%
  kable_styling()

```

It is sometimes useful to group data into different time periods to study historical data (eg. 1970s, 1980s), which we do below after removing data before 1800. Note that NASA calculates a temperature anomaly, as difference from the base period of 1951-1980. 

```{r intervals}
#The code below creates a new data frame called `comparison` that groups data 
#in five time periods: 1881-1920, 1921-1950, 1951-1980, 1981-2010 and 2011-present. 

comparison <- tidyweather %>% 
  filter(Year>= 1881) %>%     #remove years prior to 1881
  #create new variable 'interval', and assign values based on criteria below:
  mutate(interval = case_when(
    Year %in% c(1881:1920) ~ "1881-1920",
    Year %in% c(1921:1950) ~ "1921-1950",
    Year %in% c(1951:1980) ~ "1951-1980",
    Year %in% c(1981:2010) ~ "1981-2010",
    TRUE ~ "2011-present" ))
  
  comparison 
  
  comparison %>%
  rename("Delta" = "delta" , 'Full Date' = 'date',
         'Months' = 'month','Years' = 'year' ,
         'Interval' = 'interval')

```
Below we  create a density plot to study the distribution of monthly deviations (`delta`), grouped by the different time periods we are interested in. 

```{r density_plot}

comparison%>%
group_by(interval)%>%
ggplot(aes(x=delta, fill=interval)) +
  geom_density(alpha=0.2) +   
  theme_bw() +               
  labs (title = "Things are getting hotter!", subtitle = "Distribution of temperature deviations for each time interval", x = "Temperature Deviations", caption ="Source: NASA's Goddard Institute for Space Studies") 


```

So far, we have been working with monthly anomalies. However, below we proceed to inspect the average annual anomalies and create a scatter plot to display the results. 

```{r averaging}

#creating yearly averages
average_annual_anomaly <- tidyweather %>% 
  group_by(Year) %>%   #grouping data by Year
  
  # creating summaries for mean delta 
  # use `na.rm=TRUE` to eliminate NA (not available) values 
  summarise(annual_average_delta = mean(delta, na.rm=TRUE)) 

#plotting the data:
ggplot(average_annual_anomaly, aes(x=Year, y= annual_average_delta))+
  geom_point()+
  
  #Fit the best fit line, using LOESS method
  geom_smooth() +
  
  #change to theme_bw() to have white background + black frame around plot
  theme_bw() +
  labs (title = "Does the weather get more unpredictable as the years go by?", subtitle = "Average temperature deviation per year", y = "Average Annual Temperature Deviation",  caption ="Source: NASA's Goddard Institute for Space Studies")                         


```

## Confidence Interval for Temperature Anomalies 

[NASA points out on their website](https://earthobservatory.nasa.gov/world-of-change/decadaltemp.php) that 

> A one-degree global change is significant because it takes a vast amount of heat to warm all the oceans, atmosphere, and land by that much. In the past, a one- to two-degree drop was all it took to plunge the Earth into the Little Ice Age.

Below we construct a confidence interval for the average annual delta since 2011 using two different methodologies. 

```{r, calculate_CI_using_formula}
formula_ci <- comparison %>% 
  # choose the interval 2011-present
  filter(interval=="2011-present") %>% 
  summarise(mean=mean(delta,na.rm=TRUE),sd=sd(delta,na.rm=TRUE), count=n(),se=sd/sqrt(count)) %>%
  summarise(lower_ci = mean-1.96*se, upper_ci = mean+1.96*se) %>%
  rename("Lower Boundary" = 'lower_ci', "Upper Boundary" = 'upper_ci' ) %>%
kbl(caption = 'Confidence Interval for the annual delta since 2011')%>%
  kable_styling() 

formula_ci 
```


```{r, calculate_CI_using_bootstrap}
bootstrap_delta <- comparison %>% 
  filter(interval=="2011-present") %>% 
  specify(response=delta) %>%
  generate(reps =1000, type ="bootstrap") %>%
  calculate(stat ="mean")
infer_ci <- bootstrap_delta %>% 
  get_confidence_interval(level=0.95,type="percentile") %>%
  rename("Lower Boundary" = 'lower_ci', "Upper Boundary" = 'upper_ci' ) %>%
  kbl(caption = 'Confidence Interval for the annual delta since 2011')%>%
  kable_styling()

infer_ci
```
In the first code block, we manually calculate the confidence interval using the SE, count and mean. Since we are assuming normal distribution, we use the critical Z value for 95%, which is 1.96. In the second code block, we have bootstrapped the data by sampling 1000 times and calculated the mean from all these sampled sets. We then obtain a confidence interval for the mean delta. As seen from above, the two results are very similar. We are confident that in 95% of the time, the average delta will be in between 0.916 and 1.02. 

